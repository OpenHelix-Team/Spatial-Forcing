# Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model
<div align="center">

[![Paper](https://img.shields.io/badge/Paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2509.09372) [![Page](https://img.shields.io/badge/Project--Page-blue?style=for-the-badge&logo=homepage&logoColor=white)](https://github.com/OpenHelix-Team/VLA-Adapter/issues/1) [![Hugging Face Collection](https://img.shields.io/badge/Models-fcd022?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/VLA-Adapter)

</div>
<!-- TODO ÊõøÊç¢Êéâ‰∏â‰∏™ÈìæÊé• -->


## üìÉ Overview
![teaser](./figs/teaser.png)

Our model aligns the intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. This alignment demonstrates enhanced performance, training efficiency, and data effificency.


## üöÄ Get Started
Please follow our documentation step by step.

1. [**Environment Setup.**](./docs/setup.md)
2. [**Data Preparation.**](./docs/data_preparation.md)
3. [**Training and Inference.**](./docs/training_inference.md)
4. [**Deployment.**](./docs/deploy.md)


## üî• Currently Supported Features
- [x] Training and inference code on LIBERO
- [x] Checkpoints on LIBERO
- [] Deployment code
- [] Code and checkpoints on RoboTwin


## üåè Contact
For further discussion and collaboration, please feel free to contact us via Email and WeChat:

| Author | Email | WeChat |
|:---:|:---:|:---:|
| Fuhao Li | lfh23@mails.tsinghua.edu.cn | haofuly |
| Wenxuan Song | songwenxuan0115@gmail.com | swx0757 |


## ‚ù§Ô∏è Acknowledgement
We thank these great works and open-source codebases: [OpenVLA-OFT](https://github.com/moojink/openvla-oft) & [VGGT](https://github.com/facebookresearch/vggt) & [REPA](https://github.com/sihyun-yu/REPA)


## üñä Citation
If you find this work useful, please cite:

```bibtex
@article{song2025reconvla,
  title={ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver},
  author={Song, Wenxuan and Zhou, Ziyang and Zhao, Han and Chen, Jiayi and Ding, Pengxiang and Yan, Haodong and Huang, Yuxin and Tang, Feilong and Wang, Donglin and Li, Haoang},
  journal={arXiv preprint arXiv:2508.10333},
  year={2025}
}
```
<!-- todo Êõ¥Êñ∞bib -->